{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff44ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] OpenFDA:  nodes=100,495, edges=160,229\n",
      "[load] Hetionet: nodes=47,033, edges=1,379,933\n",
      "[normalize_openfda] kept=160,144 dropped(metaedge)=85 flipped=135\n",
      "[normalize_hetionet] kept=1,379,932 dropped(metaedge)=1 renamed(drug_drug->similarity)=6,486\n",
      "[collapse:drugs] considered=55,430 exact=769 fuzzy=0 skipped=54,661\n",
      "\n",
      "=== DONE ===\n",
      "• Combined PKL:      /home/ubuntu/myproject/venv/merged_ofda_hetionet/combined_openfda_hetionet.pkl\n",
      "• Collapse map CSV:  /home/ubuntu/myproject/venv/merged_ofda_hetionet/collapsed_openfda_to_hetionet.csv  (rows=769)\n",
      "• Merge report JSON: /home/ubuntu/myproject/venv/merged_ofda_hetionet/merge_report.json\n",
      "\n",
      "Final summary:\n",
      "{\n",
      "  \"hetionet_norm\": {\n",
      "    \"nodes\": 47033,\n",
      "    \"edges\": 1379932\n",
      "  },\n",
      "  \"openfda_norm\": {\n",
      "    \"nodes\": 100495,\n",
      "    \"edges\": 160144\n",
      "  },\n",
      "  \"collapsed_pairs\": 769,\n",
      "  \"final\": {\n",
      "    \"nodes\": 146759,\n",
      "    \"edges\": 1401041\n",
      "  },\n",
      "  \"final_nodes_by_type\": {\n",
      "    \"anatomy\": 402,\n",
      "    \"biological_process\": 11381,\n",
      "    \"cellular_component\": 1391,\n",
      "    \"drug\": 56558,\n",
      "    \"disease\": 1216,\n",
      "    \"gene/protein\": 20945,\n",
      "    \"molecular_function\": 2884,\n",
      "    \"pathway\": 1822,\n",
      "    \"effect/phenotype\": 33645,\n",
      "    \"unknown\": 2,\n",
      "    \"exposure\": 16513\n",
      "  },\n",
      "  \"final_edges_by_relation\": {\n",
      "    \"anatomy_gene\": 587829,\n",
      "    \"drug_effect\": 70452,\n",
      "    \"drug_exposure\": 27060,\n",
      "    \"drug_drug_similarity\": 6486,\n",
      "    \"drug_disease\": 1285,\n",
      "    \"drug_gene\": 51364,\n",
      "    \"drug_has_component\": 59253,\n",
      "    \"contraindication\": 3132,\n",
      "    \"disease_gene\": 27700,\n",
      "    \"disease_phenotype\": 3357,\n",
      "    \"disease_anatomy\": 3602,\n",
      "    \"disease_disease\": 543,\n",
      "    \"protein_protein\": 147164,\n",
      "    \"gene_gene\": 61690,\n",
      "    \"gene_pathway\": 84372,\n",
      "    \"gene_regulates_gene\": 265672,\n",
      "    \"indication\": 80\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ===== OpenFDA + Hetionet MERGE (Colab one-cell; GraphML-safe) =====\n",
    "# 1) SET THESE PATHS:\n",
    "OPENFDA_GRAPHML = \"/home/ubuntu/myproject/venv/drug_data_kg_openfda_edges_no_drop.graphml\"        # <-- change to your file\n",
    "HETIONET_GRAPHML = \"/home/ubuntu/myproject/venv/hetionet_normalized.graphml\"      # <-- change to your file\n",
    "OUTDIR = \"/home/ubuntu/myproject/venv/merged_ofda_hetionet\"            # <-- change if you like\n",
    "\n",
    "# (Optional) dataset tags to stamp into nodes/edges\n",
    "OPENFDA_VERSION = \"openfda_YYYYMMDD\"\n",
    "HETIONET_VERSION = \"hetionet_vX.Y\"\n",
    "\n",
    "# 2) Collapse policy (safe defaults)\n",
    "COLLAPSE_DRUGS = True\n",
    "COLLAPSE_DISEASES = False        # turn on only if you truly have curated names/IDs\n",
    "COLLAPSE_PHENOTYPES = False      # turn on only if you truly have curated names/IDs\n",
    "\n",
    "# Drug collapse thresholds/guards\n",
    "DRUG_TIER3_MIN_SCORE = 0.97      # fuzzy unique match minimum\n",
    "DRUG_TIER3_MIN_MARGIN = 0.02     # winner - runner-up margin\n",
    "PRODUCT_LIKE_MAX_COMPONENTS = 1  # >1 components => product-like (don't collapse)\n",
    "\n",
    "# Disease/phenotype collapse thresholds (stricter)\n",
    "DIS_TIER3_MIN_SCORE = 0.98\n",
    "DIS_TIER3_MIN_MARGIN = 0.03\n",
    "\n",
    "# Attribute keys in your GraphMLs\n",
    "NODE_TYPE_KEY = \"node_type\"\n",
    "EDGE_REL_KEY  = \"relation\"\n",
    "NODE_NAME_KEYS = [\"node_name\", \"name\", \"label\"]   # tried in this order\n",
    "\n",
    "# 3) Outputs: choose formats (PKL is always written; others are optional & GraphML-safe)\n",
    "WRITE_GRAPHML = False     # set True if you need GraphML (slower; attributes JSON-stringified)\n",
    "WRITE_GEXF    = False      # GEXF is generally faster and also string-safe here\n",
    "\n",
    "# --- (Optional) Mount Google Drive if your files live there ---\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# --- Install deps (quiet) ---\n",
    "!pip -q install networkx lxml\n",
    "\n",
    "# -------------------- PIPELINE (no edits needed below) --------------------\n",
    "import os, re, json, pickle, csv, random\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Any, Tuple, List\n",
    "import networkx as nx\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "random.seed(42)\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def load_graphml_as_multidigraph(path: str) -> nx.MultiDiGraph:\n",
    "    G = nx.read_graphml(path)\n",
    "    if not G.is_directed():\n",
    "        G = G.to_directed()\n",
    "    if not isinstance(G, (nx.MultiDiGraph, nx.MultiGraph)):\n",
    "        G = nx.MultiDiGraph(G)\n",
    "    return G\n",
    "\n",
    "def first_nonempty(d, keys=NODE_NAME_KEYS):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None and str(d[k]).strip():\n",
    "            return str(d[k])\n",
    "    return \"\"\n",
    "\n",
    "# ----- Normalization helpers -----\n",
    "FORMULATION_TOKENS = {\n",
    "    \"tablet\",\"tablets\",\"tab\",\"tabs\",\"capsule\",\"capsules\",\"cap\",\"caps\",\"cream\",\"ointment\",\"gel\",\"lozenge\",\"suspension\",\n",
    "    \"solution\",\"spray\",\"lotion\",\"patch\",\"injection\",\"syrup\",\"elixir\",\"drops\",\"chewable\",\"extended\",\"release\",\"er\",\"xr\",\"xl\",\"dr\",\"cr\",\n",
    "    \"immediate\",\"delayed\",\"topical\",\"oral\",\"nasal\",\"ophthalmic\",\"otic\",\"rectal\",\"vaginal\",\"sublingual\",\n",
    "    \"mg\",\"mcg\",\"g\",\"gram\",\"grams\",\"ml\",\"%\",\"iu\",\"spf\",\"wt\",\"vol\",\"per\",\"dose\",\"strength\",\"unit\",\"units\"\n",
    "}\n",
    "DELIMS_FOR_MIXTURES = [\",\",\"/\",\"+\",\" and \",\" with \"]\n",
    "\n",
    "def norm_name_basic(s: str) -> str:\n",
    "    s = s.upper().strip()\n",
    "    s = re.sub(r\"[^A-Z0-9 ,/+]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize_clean(s: str) -> List[str]:\n",
    "    s = norm_name_basic(s)\n",
    "    tokens = [t for t in re.split(r\"[ ,/+]\", s) if t]\n",
    "    tokens = [t for t in tokens if not t.isdigit()]\n",
    "    tokens = [t for t in tokens if t.lower() not in FORMULATION_TOKENS]\n",
    "    return tokens\n",
    "\n",
    "def jaccard(a: List[str], b: List[str]) -> float:\n",
    "    A, B = set(a), set(b)\n",
    "    return 0.0 if not A or not B else len(A & B) / len(A | B)\n",
    "\n",
    "def fuzzy_ratio(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, norm_name_basic(a), norm_name_basic(b)).ratio()\n",
    "\n",
    "def combined_score(name_a: str, name_b: str) -> float:\n",
    "    # blend sequence + token Jaccard (weights tuned conservatively)\n",
    "    t = jaccard(tokenize_clean(name_a), tokenize_clean(name_b))\n",
    "    f = fuzzy_ratio(name_a, name_b)\n",
    "    return 0.6 * f + 0.4 * t\n",
    "\n",
    "def is_mixture_like(name: str) -> bool:\n",
    "    s = norm_name_basic(name)\n",
    "    return any(d in s for d in DELIMS_FOR_MIXTURES)\n",
    "\n",
    "# ----- OpenFDA normalization -----\n",
    "def normalize_openfda(G: nx.MultiDiGraph) -> nx.MultiDiGraph:\n",
    "    out = nx.MultiDiGraph()\n",
    "    for n, d in G.nodes(data=True):\n",
    "        dd = dict(d)\n",
    "        dd[\"node_source\"] = \"openfda\"\n",
    "        dd[\"node_name\"] = first_nonempty(dd)\n",
    "        out.add_node(n, **dd)\n",
    "\n",
    "    kept = dropped = flipped = 0\n",
    "    for u, v, ed in G.edges(data=True):\n",
    "        rel = str(ed.get(EDGE_REL_KEY, \"unknown\"))\n",
    "        su = str(G.nodes[u].get(NODE_TYPE_KEY, \"unknown\"))\n",
    "        sv = str(G.nodes[v].get(NODE_TYPE_KEY, \"unknown\"))\n",
    "        ed2 = dict(ed)\n",
    "        ed2[\"edge_source\"] = \"openfda\"\n",
    "        ed2[\"display_relation\"] = rel\n",
    "\n",
    "        if rel == \"metaedge\":\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        if rel == \"drug_drug\":\n",
    "            ed2[\"relation\"] = \"drug_has_component\"\n",
    "            out.add_edge(u, v, **ed2); kept += 1; continue\n",
    "\n",
    "        if rel == \"drug_effect\":\n",
    "            if su == \"drug\" and sv == \"effect/phenotype\":\n",
    "                ed2[\"relation\"] = \"drug_effect\"\n",
    "                out.add_edge(u, v, **ed2); kept += 1; continue\n",
    "            if su == \"drug\" and sv == \"exposure\":\n",
    "                ed2[\"relation\"] = \"drug_exposure\"\n",
    "                out.add_edge(u, v, **ed2); kept += 1; continue\n",
    "            if su == \"effect/phenotype\" and sv == \"drug\":\n",
    "                ed2[\"relation\"] = \"drug_effect\"\n",
    "                ed2.setdefault(\"attrs\", {})\n",
    "                if not isinstance(ed2[\"attrs\"], dict): ed2[\"attrs\"] = {\"_raw_attrs\": ed2[\"attrs\"]}\n",
    "                ed2[\"attrs\"][\"method\"] = \"flipped\"\n",
    "                out.add_edge(v, u, **ed2); kept += 1; flipped += 1; continue\n",
    "            if su == \"effect/phenotype\" and sv in (\"effect/phenotype\",\"exposure\"):\n",
    "                ed2[\"relation\"] = \"drug_effect\"\n",
    "                ed2.setdefault(\"attrs\", {})\n",
    "                if not isinstance(ed2[\"attrs\"], dict): ed2[\"attrs\"] = {\"_raw_attrs\": ed2[\"attrs\"]}\n",
    "                ed2[\"attrs\"][\"noncanonical\"] = True\n",
    "                ed2[\"attrs\"][\"variant\"] = \"effect_to_effect\" if sv == \"effect/phenotype\" else \"effect_to_exposure\"\n",
    "                out.add_edge(u, v, **ed2); kept += 1; continue\n",
    "            ed2[\"relation\"] = \"drug_effect\"\n",
    "            out.add_edge(u, v, **ed2); kept += 1; continue\n",
    "\n",
    "        if rel in (\"indication\",\"contraindication\",\"drug_disease\"):\n",
    "            ed2[\"relation\"] = rel\n",
    "            out.add_edge(u, v, **ed2); kept += 1; continue\n",
    "\n",
    "        ed2[\"relation\"] = rel\n",
    "        out.add_edge(u, v, **ed2); kept += 1\n",
    "\n",
    "    print(f\"[normalize_openfda] kept={kept:,} dropped(metaedge)={dropped:,} flipped={flipped:,}\")\n",
    "    return out\n",
    "\n",
    "# ----- Hetionet normalization -----\n",
    "def normalize_hetionet(G: nx.MultiDiGraph) -> nx.MultiDiGraph:\n",
    "    out = nx.MultiDiGraph()\n",
    "    kept = dropped = renamed = 0\n",
    "    for n, d in G.nodes(data=True):\n",
    "        dd = dict(d)\n",
    "        dd[\"node_source\"] = \"hetionet\"\n",
    "        dd[\"node_name\"] = first_nonempty(dd)\n",
    "        out.add_node(n, **dd)\n",
    "    for u, v, ed in G.edges(data=True):\n",
    "        rel = str(ed.get(EDGE_REL_KEY, \"unknown\"))\n",
    "        if rel == \"metaedge\":\n",
    "            dropped += 1; continue\n",
    "        ed2 = dict(ed)\n",
    "        ed2[\"edge_source\"] = \"hetionet\"\n",
    "        if rel == \"drug_drug\":\n",
    "            ed2[\"relation\"] = \"drug_drug_similarity\"\n",
    "            ed2[\"display_relation\"] = ed.get(\"display_relation\", \"CrC\")\n",
    "            renamed += 1\n",
    "        else:\n",
    "            ed2[\"relation\"] = rel\n",
    "            ed2.setdefault(\"display_relation\", rel)\n",
    "        out.add_edge(u, v, **ed2); kept += 1\n",
    "    print(f\"[normalize_hetionet] kept={kept:,} dropped(metaedge)={dropped:,} renamed(drug_drug->similarity)={renamed:,}\")\n",
    "    return out\n",
    "\n",
    "# ----- Name indices for collapse -----\n",
    "def build_name_index(G: nx.MultiDiGraph, node_type: str) -> Dict[str, Dict[str, Any]]:\n",
    "    idx = {}\n",
    "    for n, d in G.nodes(data=True):\n",
    "        if str(d.get(NODE_TYPE_KEY, \"\")) != node_type: continue\n",
    "        nm = first_nonempty(d)\n",
    "        if nm: idx[n] = {\"name\": nm, \"tokens\": tokenize_clean(nm)}\n",
    "    return idx\n",
    "\n",
    "def product_like_by_structure(ofdaG: nx.MultiDiGraph, node_id: str) -> bool:\n",
    "    # if it has > PRODUCT_LIKE_MAX_COMPONENTS outgoing composition edges, treat as product-like\n",
    "    comps = 0\n",
    "    if ofdaG.has_node(node_id):\n",
    "        for _, v, ed in ofdaG.out_edges(node_id, data=True):\n",
    "            if ed.get(\"relation\") == \"drug_has_component\":\n",
    "                comps += 1\n",
    "                if comps > PRODUCT_LIKE_MAX_COMPONENTS: return True\n",
    "    return False\n",
    "\n",
    "def build_drug_collapse_map(ofdaG: nx.MultiDiGraph, hetG: nx.MultiDiGraph) -> Dict[str, str]:\n",
    "    ofda_idx = build_name_index(ofdaG, \"drug\")\n",
    "    het_idx  = build_name_index(hetG,  \"drug\")\n",
    "    het_by_exact = defaultdict(list)\n",
    "    for hid, rec in het_idx.items():\n",
    "        het_by_exact[norm_name_basic(rec[\"name\"])].append(hid)\n",
    "\n",
    "    mapping = {}\n",
    "    considered = tier2 = tier3 = skipped = 0\n",
    "\n",
    "    for oid, rec in ofda_idx.items():\n",
    "        considered += 1\n",
    "        oname = rec[\"name\"]\n",
    "\n",
    "        # guard: product-like?\n",
    "        if is_mixture_like(oname) or product_like_by_structure(ofdaG, oid):\n",
    "            skipped += 1; continue\n",
    "\n",
    "        # TIER 2: exact normalized name match\n",
    "        ex = norm_name_basic(oname)\n",
    "        if ex in het_by_exact and len(het_by_exact[ex]) == 1:\n",
    "            mapping[oid] = het_by_exact[ex][0]; tier2 += 1; continue\n",
    "\n",
    "        # TIER 3: fuzzy unique match\n",
    "        best_id, best_score, second = None, -1.0, -1.0\n",
    "        for hid, hrec in het_idx.items():\n",
    "            sc = combined_score(oname, hrec[\"name\"])\n",
    "            if sc > best_score:\n",
    "                second = best_score; best_score = sc; best_id = hid\n",
    "            elif sc > second:\n",
    "                second = sc\n",
    "        if best_id is not None and best_score >= DRUG_TIER3_MIN_SCORE and (best_score - second) >= DRUG_TIER3_MIN_MARGIN:\n",
    "            mapping[oid] = best_id; tier3 += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "    print(f\"[collapse:drugs] considered={considered:,} exact={tier2:,} fuzzy={tier3:,} skipped={skipped:,}\")\n",
    "    return mapping\n",
    "\n",
    "def build_other_collapse_map(ofdaG: nx.MultiDiGraph, hetG: nx.MultiDiGraph, node_type: str,\n",
    "                             min_score=0.98, min_margin=0.03) -> Dict[str, str]:\n",
    "    ofda_idx = build_name_index(ofdaG, node_type)\n",
    "    het_idx  = build_name_index(hetG,  node_type)\n",
    "    het_by_exact = defaultdict(list)\n",
    "    for hid, rec in het_idx.items():\n",
    "        het_by_exact[norm_name_basic(rec[\"name\"])].append(hid)\n",
    "    mapping = {}\n",
    "    considered = tier2 = tier3 = skipped = 0\n",
    "    for oid, rec in ofda_idx.items():\n",
    "        considered += 1\n",
    "        oname = rec[\"name\"]\n",
    "        ex = norm_name_basic(oname)\n",
    "        if ex in het_by_exact and len(het_by_exact[ex]) == 1:\n",
    "            mapping[oid] = het_by_exact[ex][0]; tier2 += 1; continue\n",
    "        # fuzzy\n",
    "        best_id, best_score, second = None, -1.0, -1.0\n",
    "        for hid, hrec in het_idx.items():\n",
    "            sc = combined_score(oname, hrec[\"name\"])\n",
    "            if sc > best_score:\n",
    "                second = best_score; best_score = sc; best_id = hid\n",
    "            elif sc > second:\n",
    "                second = sc\n",
    "        if best_id is not None and best_score >= min_score and (best_score - second) >= min_margin:\n",
    "            mapping[oid] = best_id; tier3 += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "    print(f\"[collapse:{node_type}] considered={considered:,} exact={tier2:,} fuzzy={tier3:,} skipped={skipped:,}\")\n",
    "    return mapping\n",
    "\n",
    "# ----- Canonicalization & edge accumulation -----\n",
    "def add_or_merge_node(canon: Dict[str, Dict[str,Any]], gid: str, base: Dict[str, Any]):\n",
    "    if gid not in canon:\n",
    "        canon[gid] = dict(base)\n",
    "        return\n",
    "    N = canon[gid]\n",
    "    # node_source merge\n",
    "    def to_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, list): return x\n",
    "        return [x]\n",
    "    srcs = set(to_list(N.get(\"node_source\"))) | set(to_list(base.get(\"node_source\")))\n",
    "    N[\"node_source\"] = sorted(srcs) if srcs else None\n",
    "    # alias names & ids\n",
    "    N.setdefault(\"alias_names\", [])\n",
    "    if base.get(\"node_name\") and base[\"node_name\"] not in N[\"alias_names\"]:\n",
    "        N[\"alias_names\"].append(base[\"node_name\"])\n",
    "    N.setdefault(\"aliases\", [])\n",
    "    for a in to_list(base.get(\"aliases\")):\n",
    "        if a not in N[\"aliases\"]:\n",
    "            N[\"aliases\"].append(a)\n",
    "    # attrs.sources merge\n",
    "    N.setdefault(\"attrs\", {})\n",
    "    N[\"attrs\"].setdefault(\"sources\", {})\n",
    "    battrs = {k:v for k,v in base.get(\"attrs\", {}).items()}\n",
    "    src_label = base.get(\"node_source\", \"unknown\")\n",
    "    if isinstance(src_label, list): src_label = \"_\".join(src_label)\n",
    "    N[\"attrs\"][\"sources\"][src_label] = battrs\n",
    "\n",
    "def coalesce_edge_payload(cur: Dict[str, Any], newe: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    def to_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, list): return x\n",
    "        return [x]\n",
    "    cur[\"edge_source\"] = sorted(set(to_list(cur.get(\"edge_source\")) + to_list(newe.get(\"edge_source\"))))\n",
    "    cur[\"display_relation\"] = sorted(set(to_list(cur.get(\"display_relation\")) + to_list(newe.get(\"display_relation\"))))\n",
    "    cur.setdefault(\"attrs\", {})\n",
    "    cur[\"attrs\"].setdefault(\"sources\", {})\n",
    "    ns = newe.get(\"edge_source\", \"unknown\")\n",
    "    if isinstance(ns, list): ns = \"_\".join(ns)\n",
    "    cur[\"attrs\"][\"sources\"][ns] = newe.get(\"attrs\", {})\n",
    "    return cur\n",
    "\n",
    "# ----- GraphML/GEXF-safe copier (JSON-stringify non-scalars) -----\n",
    "def _graphml_safe_val(v):\n",
    "    if v is None: return \"\"\n",
    "    if isinstance(v, (str, int, float, bool)): return v\n",
    "    try:\n",
    "        return json.dumps(v, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "def make_graphml_safe(G):\n",
    "    G2 = nx.MultiDiGraph()\n",
    "    for n, d in G.nodes(data=True):\n",
    "        d2 = {k: _graphml_safe_val(v) for k, v in d.items()}\n",
    "        G2.add_node(n, **d2)\n",
    "    for u, v, ed in G.edges(data=True):\n",
    "        ed2 = {k: _graphml_safe_val(val) for k, val in ed.items()}\n",
    "        G2.add_edge(u, v, **ed2)\n",
    "    return G2\n",
    "\n",
    "# ----- Main runner -----\n",
    "def run_merge():\n",
    "    # Load\n",
    "    G_ofda_raw = load_graphml_as_multidigraph(OPENFDA_GRAPHML)\n",
    "    G_het_raw  = load_graphml_as_multidigraph(HETIONET_GRAPHML)\n",
    "    print(f\"[load] OpenFDA:  nodes={G_ofda_raw.number_of_nodes():,}, edges={G_ofda_raw.number_of_edges():,}\")\n",
    "    print(f\"[load] Hetionet: nodes={G_het_raw.number_of_nodes():,}, edges={G_het_raw.number_of_edges():,}\")\n",
    "\n",
    "    # Normalize\n",
    "    G_ofda = normalize_openfda(G_ofda_raw)\n",
    "    G_het  = normalize_hetionet(G_het_raw)\n",
    "\n",
    "    # Collapse maps\n",
    "    collapse_map = {}\n",
    "    if COLLAPSE_DRUGS:\n",
    "        collapse_map.update(build_drug_collapse_map(G_ofda, G_het))\n",
    "    if COLLAPSE_DISEASES:\n",
    "        collapse_map.update(build_other_collapse_map(G_ofda, G_het, \"disease\", DIS_TIER3_MIN_SCORE, DIS_TIER3_MIN_MARGIN))\n",
    "    if COLLAPSE_PHENOTYPES:\n",
    "        collapse_map.update(build_other_collapse_map(G_ofda, G_het, \"effect/phenotype\", DIS_TIER3_MIN_SCORE, DIS_TIER3_MIN_MARGIN))\n",
    "\n",
    "    # Canonical node table\n",
    "    canon_nodes: Dict[str, Dict[str,Any]] = {}\n",
    "    def mk_node(nid: str, d: Dict[str,Any], src_prefix: str) -> Dict[str,Any]:\n",
    "        rec = {\n",
    "            \"gid\": f\"{src_prefix}:{nid}\",\n",
    "            \"node_id\": str(nid),\n",
    "            \"node_type\": d.get(NODE_TYPE_KEY, \"unknown\"),\n",
    "            \"node_name\": first_nonempty(d),\n",
    "            \"node_source\": src_prefix,\n",
    "            \"attrs\": {k:v for k,v in d.items() if k not in (NODE_TYPE_KEY, \"node_source\", \"node_name\")}\n",
    "        }\n",
    "        if src_prefix == \"openfda\" and OPENFDA_VERSION:\n",
    "            rec[\"version\"] = OPENFDA_VERSION\n",
    "        if src_prefix == \"hetionet\" and HETIONET_VERSION:\n",
    "            rec[\"version\"] = HETIONET_VERSION\n",
    "        return rec\n",
    "\n",
    "    # Add Hetionet canonical nodes\n",
    "    for n, d in G_het.nodes(data=True):\n",
    "        canon_nodes[f\"hetionet:{n}\"] = mk_node(n, d, \"hetionet\")\n",
    "\n",
    "    # Add OpenFDA nodes (collapse when mapped)\n",
    "    collapsed = []\n",
    "    kept_openfda = []\n",
    "    for n, d in G_ofda.nodes(data=True):\n",
    "        if n in collapse_map:\n",
    "            hid = collapse_map[n]\n",
    "            canon_gid = f\"hetionet:{hid}\"\n",
    "            base = mk_node(n, d, \"openfda\")\n",
    "            base[\"aliases\"] = [base[\"node_id\"]]\n",
    "            add_or_merge_node(canon_nodes, canon_gid, base)\n",
    "            collapsed.append((n, hid, d.get(NODE_TYPE_KEY)))\n",
    "        else:\n",
    "            canon_nodes[f\"openfda:{n}\"] = mk_node(n, d, \"openfda\")\n",
    "            kept_openfda.append((n, d.get(NODE_TYPE_KEY)))\n",
    "\n",
    "    # Edge accumulation (with rewiring & dedupe)\n",
    "    edge_bucket: Dict[Tuple[str,str,str], Dict[str,Any]] = {}\n",
    "\n",
    "    def acc_edge(src_native: str, dst_native: str, ed: Dict[str,Any], is_openfda: bool):\n",
    "        if is_openfda:\n",
    "            sg = f\"hetionet:{collapse_map[src_native]}\" if src_native in collapse_map else f\"openfda:{src_native}\"\n",
    "            dg = f\"hetionet:{collapse_map[dst_native]}\" if dst_native in collapse_map else f\"openfda:{dst_native}\"\n",
    "        else:\n",
    "            sg, dg = f\"hetionet:{src_native}\", f\"hetionet:{dst_native}\"\n",
    "        rel = ed.get(\"relation\")\n",
    "        key = (sg, rel, dg)\n",
    "        payload = {\n",
    "            \"relation\": rel,\n",
    "            \"edge_source\": ed.get(\"edge_source\", \"openfda\" if is_openfda else \"hetionet\"),\n",
    "            \"display_relation\": ed.get(\"display_relation\", rel),\n",
    "            \"attrs\": {k:v for k,v in ed.items() if k not in (\"relation\",\"edge_source\",\"display_relation\")}\n",
    "        }\n",
    "        if is_openfda and OPENFDA_VERSION: payload[\"version\"] = OPENFDA_VERSION\n",
    "        if (not is_openfda) and HETIONET_VERSION: payload[\"version\"] = HETIONET_VERSION\n",
    "\n",
    "        if key in edge_bucket:\n",
    "            edge_bucket[key] = coalesce_edge_payload(edge_bucket[key], payload)\n",
    "        else:\n",
    "            edge_bucket[key] = payload\n",
    "\n",
    "    for u, v, ed in G_ofda.edges(data=True):\n",
    "        acc_edge(u, v, ed, is_openfda=True)\n",
    "    for u, v, ed in G_het.edges(data=True):\n",
    "        acc_edge(u, v, ed, is_openfda=False)\n",
    "\n",
    "    # Build final graph\n",
    "    G_final = nx.MultiDiGraph()\n",
    "    for gid, nd in canon_nodes.items():\n",
    "        G_final.add_node(gid, **nd)\n",
    "    for (su, rel, sv), pd in edge_bucket.items():\n",
    "        G_final.add_edge(su, sv, **pd)\n",
    "\n",
    "    # Write always: PKL, collapse map, JSON report\n",
    "    out_pkl     = os.path.join(OUTDIR, \"combined_openfda_hetionet.pkl\")\n",
    "    out_map_csv = os.path.join(OUTDIR, \"collapsed_openfda_to_hetionet.csv\")\n",
    "    out_report  = os.path.join(OUTDIR, \"merge_report.json\")\n",
    "    with open(out_pkl, \"wb\") as f:\n",
    "        pickle.dump(G_final, f)\n",
    "\n",
    "    with open(out_map_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f); w.writerow([\"openfda_node_id\",\"hetionet_node_id\",\"node_type\"])\n",
    "        for oid, hid, t in collapsed: w.writerow([oid, hid, t])\n",
    "\n",
    "    def count_nodes_by_type(G):\n",
    "        c = Counter(str(d.get(NODE_TYPE_KEY, \"unknown\")) for _, d in G.nodes(data=True))\n",
    "        return dict(c)\n",
    "    def count_edges_by_rel(G):\n",
    "        c = Counter(str(d.get(\"relation\",\"unknown\")) for _, _, d in G.edges(data=True))\n",
    "        return dict(c)\n",
    "\n",
    "    rep = {\n",
    "        \"inputs\": {\n",
    "            \"openfda_graphml\": OPENFDA_GRAPHML,\n",
    "            \"hetionet_graphml\": HETIONET_GRAPHML,\n",
    "            \"openfda_version\": OPENFDA_VERSION,\n",
    "            \"hetionet_version\": HETIONET_VERSION\n",
    "        },\n",
    "        \"policy\": {\n",
    "            \"collapse_drugs\": COLLAPSE_DRUGS,\n",
    "            \"collapse_diseases\": COLLAPSE_DISEASES,\n",
    "            \"collapse_phenotypes\": COLLAPSE_PHENOTYPES,\n",
    "            \"drug_tier3_min_score\": DRUG_TIER3_MIN_SCORE,\n",
    "            \"drug_tier3_min_margin\": DRUG_TIER3_MIN_MARGIN,\n",
    "            \"disease_tier3_min_score\": DIS_TIER3_MIN_SCORE,\n",
    "            \"disease_tier3_min_margin\": DIS_TIER3_MIN_MARGIN\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"hetionet_norm\": {\"nodes\": G_het.number_of_nodes(), \"edges\": G_het.number_of_edges()},\n",
    "            \"openfda_norm\": {\"nodes\": G_ofda.number_of_nodes(), \"edges\": G_ofda.number_of_edges()},\n",
    "            \"collapsed_pairs\": len(collapsed),\n",
    "            \"final\": {\"nodes\": G_final.number_of_nodes(), \"edges\": G_final.number_of_edges()},\n",
    "            \"final_nodes_by_type\": count_nodes_by_type(G_final),\n",
    "            \"final_edges_by_relation\": count_edges_by_rel(G_final)\n",
    "        }\n",
    "    }\n",
    "    with open(out_report, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rep, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Optional: GraphML/GEXF (GraphML-safe)\n",
    "    wrote_graphml = wrote_gexf = False\n",
    "    if WRITE_GRAPHML or WRITE_GEXF:\n",
    "        G_safe = make_graphml_safe(G_final)\n",
    "        if WRITE_GRAPHML:\n",
    "            out_graphml = os.path.join(OUTDIR, \"combined_openfda_hetionet.graphml\")\n",
    "            nx.write_graphml(G_safe, out_graphml)\n",
    "            wrote_graphml = True\n",
    "        if WRITE_GEXF:\n",
    "            out_gexf = os.path.join(OUTDIR, \"combined_openfda_hetionet.gexf\")\n",
    "            nx.write_gexf(G_safe, out_gexf)\n",
    "            wrote_gexf = True\n",
    "\n",
    "    print(\"\\n=== DONE ===\")\n",
    "    print(f\"• Combined PKL:      {out_pkl}\")\n",
    "    if wrote_graphml: print(f\"• Combined GraphML:  {out_graphml}\")\n",
    "    if wrote_gexf:    print(f\"• Combined GEXF:     {out_gexf}\")\n",
    "    print(f\"• Collapse map CSV:  {out_map_csv}  (rows={len(collapsed)})\")\n",
    "    print(f\"• Merge report JSON: {out_report}\")\n",
    "    print(\"\\nFinal summary:\")\n",
    "    print(json.dumps(rep[\"stats\"], indent=2))\n",
    "\n",
    "run_merge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858d2504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "📥 Loading NetworkX graph from pickle: /home/ubuntu/myproject/venv/merged_ofda_hetionet/combined_openfda_hetionet.pkl\n",
      "✅ Loaded graph with 146759 nodes and 1401041 edges.\n",
      "✅ Node types: ['anatomy', 'biological_process', 'cellular_component', 'drug', 'disease', 'gene/protein', 'molecular_function', 'pathway', 'effect/phenotype', 'unknown', 'exposure']\n",
      "✅ Total edge types: 22\n",
      "Homogeneous nodes: 146759, edges used for training: 1120824\n",
      "\n",
      "🔁 Training with NeighborLoader mini-batches + AMP ...\n",
      "Epoch 01 | Loss: 18.0152 | Seen pos edges: 106496\n",
      "Epoch 02 | Loss: 17.8158 | Seen pos edges: 106496\n",
      "Epoch 03 | Loss: 17.2474 | Seen pos edges: 106496\n",
      "Epoch 04 | Loss: 16.5901 | Seen pos edges: 106496\n",
      "Epoch 05 | Loss: 16.0009 | Seen pos edges: 106496\n",
      "Epoch 06 | Loss: 15.4065 | Seen pos edges: 106496\n",
      "Epoch 07 | Loss: 15.0668 | Seen pos edges: 106496\n",
      "Epoch 08 | Loss: 14.7636 | Seen pos edges: 106496\n",
      "Epoch 09 | Loss: 14.4805 | Seen pos edges: 106496\n",
      "Epoch 10 | Loss: 14.3493 | Seen pos edges: 106496\n",
      "Epoch 11 | Loss: 14.2180 | Seen pos edges: 106496\n",
      "Epoch 12 | Loss: 14.0697 | Seen pos edges: 106496\n",
      "Epoch 13 | Loss: 14.0017 | Seen pos edges: 106496\n",
      "Epoch 14 | Loss: 13.9264 | Seen pos edges: 106496\n",
      "Epoch 15 | Loss: 13.8970 | Seen pos edges: 106496\n",
      "Epoch 16 | Loss: 13.8409 | Seen pos edges: 106496\n",
      "Epoch 17 | Loss: 13.7839 | Seen pos edges: 106496\n",
      "Epoch 18 | Loss: 13.7529 | Seen pos edges: 106496\n",
      "Epoch 19 | Loss: 13.6724 | Seen pos edges: 106496\n",
      "Epoch 20 | Loss: 13.6614 | Seen pos edges: 106496\n",
      "Epoch 21 | Loss: 13.6263 | Seen pos edges: 106496\n",
      "Epoch 22 | Loss: 13.6075 | Seen pos edges: 106496\n",
      "Epoch 23 | Loss: 13.5328 | Seen pos edges: 106496\n",
      "Epoch 24 | Loss: 13.4955 | Seen pos edges: 106496\n",
      "Epoch 25 | Loss: 13.5415 | Seen pos edges: 106496\n",
      "Epoch 26 | Loss: 13.4620 | Seen pos edges: 106496\n",
      "Epoch 27 | Loss: 13.4491 | Seen pos edges: 106496\n",
      "Epoch 28 | Loss: 13.4150 | Seen pos edges: 106496\n",
      "Epoch 29 | Loss: 13.4060 | Seen pos edges: 106496\n",
      "Epoch 30 | Loss: 13.3777 | Seen pos edges: 106496\n",
      "\n",
      "🔎 Computing full-node embeddings (batched inference) ...\n",
      "\n",
      "📊 Final evaluation per edge type (filtered negatives, DistMult):\n",
      "('anatomy', 'anatomy_gene', 'gene/protein'): AUC=0.6484, P=0.561, R=0.848, F1=0.676\n",
      "('drug', 'drug_effect', 'effect/phenotype'): AUC=0.5344, P=0.507, R=0.095, F1=0.161\n",
      "('drug', 'drug_exposure', 'exposure'): AUC=0.4813, P=0.483, R=0.114, F1=0.185\n",
      "('drug', 'drug_drug_similarity', 'drug'): AUC=0.9724, P=0.973, R=0.878, F1=0.923\n",
      "('drug', 'drug_disease', 'disease'): AUC=0.8003, P=0.901, R=0.705, F1=0.791\n",
      "('drug', 'drug_gene', 'gene/protein'): AUC=0.9073, P=0.842, R=0.855, F1=0.848\n",
      "('drug', 'drug_has_component', 'drug'): AUC=0.6518, P=0.627, R=0.513, F1=0.565\n",
      "('drug', 'contraindication', 'disease'): AUC=0.5138, P=0.504, R=0.907, F1=0.648\n",
      "('disease', 'disease_gene', 'gene/protein'): AUC=0.6441, P=0.565, R=0.715, F1=0.631\n",
      "('disease', 'disease_phenotype', 'effect/phenotype'): AUC=0.4887, P=0.500, R=1.000, F1=0.667\n",
      "('disease', 'disease_anatomy', 'anatomy'): AUC=0.5224, P=0.514, R=0.665, F1=0.580\n",
      "('disease', 'disease_disease', 'disease'): AUC=0.6433, P=0.536, R=0.673, F1=0.597\n",
      "('gene/protein', 'protein_protein', 'gene/protein'): AUC=0.8079, P=0.750, R=0.710, F1=0.729\n",
      "('gene/protein', 'gene_gene', 'gene/protein'): AUC=0.7749, P=0.781, R=0.579, F1=0.665\n",
      "('gene/protein', 'gene_pathway', 'pathway'): AUC=0.4997, P=0.500, R=0.559, F1=0.528\n",
      "('gene/protein', 'gene_regulates_gene', 'gene/protein'): AUC=0.9172, P=0.870, R=0.797, F1=0.832\n",
      "('effect/phenotype', 'indication', 'effect/phenotype'): AUC=0.4922, P=0.500, R=1.000, F1=0.667\n",
      "('effect/phenotype', 'drug_effect', 'effect/phenotype'): AUC=0.7143, P=0.500, R=1.000, F1=0.667\n",
      "('effect/phenotype', 'contraindication', 'effect/phenotype'): AUC=0.5000, P=0.500, R=1.000, F1=0.667\n",
      "('drug', 'indication', 'disease'): AUC=0.0000, P=0.000, R=0.000, F1=0.000\n",
      "\n",
      "🌍 Overall Performance (filtered-ish, typed tails, DistMult):\n",
      "Overall AUC = 0.7422\n",
      "Overall Precision = 0.640\n",
      "Overall Recall = 0.726\n",
      "Overall F1 Score = 0.680\n"
     ]
    }
   ],
   "source": [
    "# === Pickle (NetworkX) → R-GCN (NeighborLoader + AMP + batched inference) ===\n",
    "# Fixes:\n",
    "#  - DistMult scoring (relation-aware) in train + eval\n",
    "#  - Typed negatives sampled *within the batch* (local), avoiding drop\n",
    "#  - zero_division=0 to silence undefined metric warnings\n",
    "\n",
    "import os, gc, random, pickle, gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "import networkx as nx\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Repro + Device\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gc.collect()\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load PICKLE (NetworkX) + normalize attrs\n",
    "# ----------------------------\n",
    "# Accepts a pickle produced via: pickle.dump(G, open(path,'wb')) where G is a NetworkX graph\n",
    "pickle_path = \"/home/ubuntu/myproject/venv/merged_ofda_hetionet/combined_openfda_hetionet.pkl\"  # <- change if needed\n",
    "assert os.path.exists(pickle_path), f\"Path does not exist: {pickle_path}\"\n",
    "print(f\"📥 Loading NetworkX graph from pickle: {pickle_path}\")\n",
    "\n",
    "open_fn = gzip.open if pickle_path.endswith(\".gz\") else open\n",
    "with open_fn(pickle_path, \"rb\") as f:\n",
    "    G0 = pickle.load(f)\n",
    "\n",
    "# Ensure MultiDiGraph container\n",
    "if isinstance(G0, (nx.MultiDiGraph, nx.MultiGraph, nx.Graph, nx.DiGraph)):\n",
    "    G = nx.MultiDiGraph()\n",
    "    G.add_nodes_from(G0.nodes(data=True))\n",
    "    if isinstance(G0, (nx.MultiDiGraph, nx.MultiGraph)):\n",
    "        for u, v, k, d in G0.edges(keys=True, data=True):\n",
    "            G.add_edge(u, v, key=k, **(d or {}))\n",
    "    else:\n",
    "        for u, v, d in G0.edges(data=True):\n",
    "            G.add_edge(u, v, **(d or {}))\n",
    "else:\n",
    "    raise TypeError(f\"Pickle did not contain a NetworkX graph, got {type(G0)}\")\n",
    "\n",
    "# Normalize expected attributes\n",
    "for n, data in G.nodes(data=True):\n",
    "    if \"node_type\" not in data:\n",
    "        data[\"node_type\"] = data.get(\"type\", data.get(\"category\", data.get(\"kind\", \"unknown\")))\n",
    "for _, _, attr in G.edges(data=True):\n",
    "    if \"relation\" not in attr:\n",
    "        attr[\"relation\"] = attr.get(\"display_relation\", attr.get(\"relationship\", attr.get(\"type\", attr.get(\"label\", \"unknown\"))))\n",
    "\n",
    "print(f\"✅ Loaded graph with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Typed remap + splits\n",
    "# ----------------------------\n",
    "node_type_map = defaultdict(list)\n",
    "node_id_map = {}\n",
    "for nid, data in G.nodes(data=True):\n",
    "    ntype = data.get(\"node_type\", \"unknown\")\n",
    "    idx = len(node_type_map[ntype])\n",
    "    node_type_map[ntype].append(nid)\n",
    "    node_id_map[nid] = (ntype, idx)\n",
    "\n",
    "edge_type_map = defaultdict(list)  # (src_type, rel, dst_type) -> [(src_local, dst_local)]\n",
    "for s, t, attr in G.edges(data=True):\n",
    "    if s not in node_id_map or t not in node_id_map:\n",
    "        continue\n",
    "    rel = attr.get(\"relation\", \"unknown\")\n",
    "    s_t, s_i = node_id_map[s]\n",
    "    t_t, t_i = node_id_map[t]\n",
    "    edge_type_map[(s_t, rel, t_t)].append((s_i, t_i))\n",
    "\n",
    "print(f\"✅ Node types: {list(node_type_map.keys())}\")\n",
    "print(f\"✅ Total edge types: {len(edge_type_map)}\")\n",
    "\n",
    "# Global ids per (type, local)\n",
    "type_offsets, global_id_map = {}, {}\n",
    "offset = 0\n",
    "for ntype, nodes in node_type_map.items():\n",
    "    type_offsets[ntype] = offset\n",
    "    for i in range(len(nodes)):\n",
    "        global_id_map[(ntype, i)] = offset + i\n",
    "    offset += len(nodes)\n",
    "num_nodes = offset\n",
    "\n",
    "edge_type_keys = list(edge_type_map.keys())  # rel_id -> (src_type, rel, dst_type)\n",
    "rel2id = {k: i for i, k in enumerate(edge_type_keys)}\n",
    "num_relations = len(edge_type_keys)\n",
    "\n",
    "all_src, all_dst, all_rel = [], [], []\n",
    "edge_type_splits = {}\n",
    "\n",
    "for etype, pairs in edge_type_map.items():\n",
    "    s_type, _, t_type = etype\n",
    "    if len(pairs) < 5:  # keep only relations with minimum support\n",
    "        continue\n",
    "    perm = torch.randperm(len(pairs))\n",
    "    n_tr = int(0.8 * len(pairs))\n",
    "    n_va = int(0.1 * len(pairs))\n",
    "    tr = [pairs[i] for i in perm[:n_tr]]\n",
    "    va = [pairs[i] for i in perm[n_tr:n_tr + n_va]]\n",
    "    te = [pairs[i] for i in perm[n_tr + n_va:]]\n",
    "\n",
    "    r = rel2id[etype]\n",
    "    for s_l, t_l in tr:\n",
    "        all_src.append(type_offsets[s_type] + s_l)\n",
    "        all_dst.append(type_offsets[t_type] + t_l)\n",
    "        all_rel.append(r)\n",
    "\n",
    "    edge_type_splits[etype] = {\n",
    "        \"val\": torch.tensor(\n",
    "            [[type_offsets[s_type] + s for s, _ in va],\n",
    "             [type_offsets[t_type] + t for _, t in va]], dtype=torch.long),\n",
    "        \"test\": torch.tensor(\n",
    "            [[type_offsets[s_type] + s for s, _ in te],\n",
    "             [type_offsets[t_type] + t for _, t in te]], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "edge_index_cpu = torch.tensor([all_src, all_dst], dtype=torch.long)\n",
    "edge_type_cpu = torch.tensor(all_rel, dtype=torch.long)\n",
    "print(f\"Homogeneous nodes: {num_nodes}, edges used for training: {edge_index_cpu.size(1)}\")\n",
    "\n",
    "# For quick type lookup by GLOBAL id (used for local typed negatives)\n",
    "type_names = sorted(node_type_map.keys())\n",
    "type2id = {t: i for i, t in enumerate(type_names)}\n",
    "global_node_type_id = torch.empty(num_nodes, dtype=torch.long)\n",
    "for tname, nodes in node_type_map.items():\n",
    "    t_id = type2id[tname]\n",
    "    base = type_offsets[tname]\n",
    "    n = len(nodes)\n",
    "    global_node_type_id[base:base + n] = t_id\n",
    "\n",
    "# Existing edges per relation (LOCAL to each (src_type,rel,dst_type))\n",
    "existing_edges_per_rel = {etype: set(pairs) for etype, pairs in edge_type_map.items()}\n",
    "def relid_to_etype(rel_id: int):\n",
    "    return edge_type_keys[rel_id]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) PyG Data + NeighborLoader\n",
    "# ----------------------------\n",
    "data_pyg = Data(num_nodes=num_nodes, edge_index=edge_index_cpu, edge_type=edge_type_cpu)\n",
    "\n",
    "NUM_NEIGHBORS = [15, 10]\n",
    "NBATCH_SIZE   = 4096\n",
    "NUM_WORKERS   = 2  # quiets the OS warning\n",
    "\n",
    "train_src_unique = torch.unique(edge_index_cpu[0])\n",
    "train_loader = NeighborLoader(\n",
    "    data_pyg,\n",
    "    num_neighbors=NUM_NEIGHBORS,\n",
    "    batch_size=NBATCH_SIZE,\n",
    "    input_nodes=train_src_unique,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Model (DistMult) + helpers\n",
    "# ----------------------------\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, num_relations, emb_dim=64, hidden_dim=128, out_dim=32, num_bases=30):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_nodes, emb_dim)\n",
    "        self.conv1 = RGCNConv(emb_dim, hidden_dim, num_relations=num_relations, num_bases=num_bases)\n",
    "        self.conv2 = RGCNConv(hidden_dim, out_dim, num_relations=num_relations, num_bases=num_bases)\n",
    "        self.rel_emb = nn.Embedding(num_relations, out_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
    "\n",
    "    def forward(self, n_id, edge_index, edge_type):\n",
    "        # n_id: global ids ordered as local nodes in the subgraph\n",
    "        x = self.emb(n_id)\n",
    "        x = self.conv1(x, edge_index, edge_type).relu_()\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x  # local node embeddings\n",
    "\n",
    "    def score(self, h, r, t):\n",
    "        # DistMult (logits)\n",
    "        return (h * r * t).sum(dim=-1)\n",
    "\n",
    "def distmult_sigmoid_scores(z, rel_emb, eidx, rel_ids):\n",
    "    h = z[eidx[0]]; t = z[eidx[1]]\n",
    "    r = rel_emb(rel_ids)\n",
    "    return (h * r * t).sum(dim=-1).sigmoid()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Local (in-batch) typed negatives\n",
    "# ----------------------------\n",
    "def build_local_type_pools(n_id_device):\n",
    "    \"\"\"Map node_type_id -> list of LOCAL indices present in this batch.\"\"\"\n",
    "    g_types = global_node_type_id.to(n_id_device.device, non_blocking=True)[n_id_device]\n",
    "    pools = defaultdict(list)\n",
    "    for local_idx, t in enumerate(g_types.tolist()):\n",
    "        pools[t].append(local_idx)\n",
    "    for k in list(pools.keys()):\n",
    "        pools[k] = torch.tensor(pools[k], dtype=torch.long, device=n_id_device.device)\n",
    "    return pools\n",
    "\n",
    "def sample_neg_local(pos_local_eidx, rel_ids_local, n_id, pools, k_neg=1):\n",
    "    \"\"\"\n",
    "    Typed negatives *within* the sampled subgraph.\n",
    "    pos_local_eidx: (2,P) local indices\n",
    "    rel_ids_local:  (P,)   relation ids (global rel ids)\n",
    "    pools: dict[type_id] -> local node index tensor present in batch\n",
    "    \"\"\"\n",
    "    if pos_local_eidx.numel() == 0:  # empty\n",
    "        return torch.empty((2,0), dtype=torch.long, device=pos_local_eidx.device)\n",
    "\n",
    "    P = pos_local_eidx.size(1)\n",
    "    neg_src, neg_dst = [], []\n",
    "    for i in range(P):\n",
    "        s_l = int(pos_local_eidx[0, i])\n",
    "        d_l = int(pos_local_eidx[1, i])\n",
    "        r_id = int(rel_ids_local[i])\n",
    "        s_type, _, d_type = relid_to_etype(r_id)\n",
    "        s_tid = type2id[s_type]; d_tid = type2id[d_type]\n",
    "\n",
    "        # pick from local pools — if missing (rare), fall back to uniform local sampling\n",
    "        s_pool = pools.get(s_tid, None)\n",
    "        d_pool = pools.get(d_tid, None)\n",
    "        if s_pool is None or s_pool.numel() == 0:\n",
    "            s_pool = torch.arange(n_id.size(0), device=n_id.device)\n",
    "        if d_pool is None or d_pool.numel() == 0:\n",
    "            d_pool = torch.arange(n_id.size(0), device=n_id.device)\n",
    "\n",
    "        for _ in range(max(1, k_neg)):\n",
    "            if random.random() < 0.5:\n",
    "                # corrupt head\n",
    "                new_s_l = s_pool[torch.randint(0, s_pool.numel(), (1,))].item()\n",
    "                neg_src.append(new_s_l); neg_dst.append(d_l)\n",
    "            else:\n",
    "                # corrupt tail\n",
    "                new_d_l = d_pool[torch.randint(0, d_pool.numel(), (1,))].item()\n",
    "                neg_src.append(s_l); neg_dst.append(new_d_l)\n",
    "\n",
    "    if len(neg_src) == 0:\n",
    "        return torch.empty((2,0), dtype=torch.long, device=n_id.device)\n",
    "    return torch.stack([torch.tensor(neg_src, device=n_id.device),\n",
    "                        torch.tensor(neg_dst, device=n_id.device)], dim=0)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Train with NeighborLoader + AMP\n",
    "# ----------------------------\n",
    "EPOCHS = 30\n",
    "LR     = 1e-3\n",
    "K_NEG  = 1\n",
    "\n",
    "model = RGCN(num_nodes, num_relations, emb_dim=64, hidden_dim=128, out_dim=32, num_bases=30).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "# Global training edges (for picking positives per batch)\n",
    "src_glob_all = edge_index_cpu[0].to(device)\n",
    "dst_glob_all = edge_index_cpu[1].to(device)\n",
    "rels_all     = edge_type_cpu.to(device)\n",
    "\n",
    "print(\"\\n🔁 Training with NeighborLoader mini-batches + AMP ...\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    seen_pos = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        n_id = batch.n_id  # global ids in this subgraph (device tensor)\n",
    "\n",
    "        # global->local mapping\n",
    "        global2local = -torch.ones(num_nodes, dtype=torch.long, device=device)\n",
    "        global2local[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # positives inside batch\n",
    "        in_src = global2local[src_glob_all]\n",
    "        in_dst = global2local[dst_glob_all]\n",
    "        mask = (in_src >= 0) & (in_dst >= 0)\n",
    "        pos_idx = torch.nonzero(mask, as_tuple=False).view(-1)\n",
    "        if pos_idx.numel() == 0:\n",
    "            del batch, n_id, global2local, in_src, in_dst, mask, pos_idx\n",
    "            if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        MAX_POS_PER_BATCH = 8192\n",
    "        if pos_idx.numel() > MAX_POS_PER_BATCH:\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device)[:MAX_POS_PER_BATCH]]\n",
    "\n",
    "        pos_local = torch.stack([in_src[pos_idx], in_dst[pos_idx]], dim=0)\n",
    "        rel_ids_local = rels_all[pos_idx]\n",
    "\n",
    "        # local typed pools for negatives\n",
    "        pools = build_local_type_pools(n_id)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=(device.type == \"cuda\")):\n",
    "            z = model(n_id, batch.edge_index, batch.edge_type)\n",
    "\n",
    "            # DistMult positives\n",
    "            pos_h = z[pos_local[0]]; pos_t = z[pos_local[1]]\n",
    "            pos_r = model.rel_emb(rel_ids_local)\n",
    "            pos_logits = model.score(pos_h, pos_r, pos_t)\n",
    "\n",
    "            # local typed negatives\n",
    "            neg_local = sample_neg_local(pos_local, rel_ids_local, n_id, pools, k_neg=K_NEG)\n",
    "            if neg_local.size(1) > 0:\n",
    "                neg_h = z[neg_local[0]]; neg_t = z[neg_local[1]]\n",
    "                # repeat relation ids for negs\n",
    "                neg_r = model.rel_emb(rel_ids_local.repeat_interleave(K_NEG))\n",
    "                neg_logits = model.score(neg_h, neg_r, neg_t)\n",
    "                loss = (F.binary_cross_entropy_with_logits(pos_logits, torch.ones_like(pos_logits)) +\n",
    "                        F.binary_cross_entropy_with_logits(neg_logits, torch.zeros_like(neg_logits)))\n",
    "            else:\n",
    "                loss = F.binary_cross_entropy_with_logits(pos_logits, torch.ones_like(pos_logits))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += float(loss.detach().cpu())\n",
    "        seen_pos += pos_local.size(1)\n",
    "\n",
    "        # cleanup\n",
    "        del batch, n_id, global2local, in_src, in_dst, mask, pos_idx\n",
    "        del pos_local, rel_ids_local, pools, z, pos_h, pos_t, pos_r, pos_logits\n",
    "        if 'neg_local' in locals(): del neg_local, neg_h, neg_t, neg_r, neg_logits\n",
    "        if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {epoch_loss:.4f} | Seen pos edges: {seen_pos}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Batched inference (DistMult-ready embeddings)\n",
    "# ----------------------------\n",
    "print(\"\\n🔎 Computing full-node embeddings (batched inference) ...\")\n",
    "model.eval()\n",
    "OUT_DIM = 32\n",
    "z_all = torch.empty((num_nodes, OUT_DIM), dtype=torch.float32, device=device)\n",
    "\n",
    "INFER_BATCH = 16384\n",
    "infer_loader = NeighborLoader(\n",
    "    Data(num_nodes=num_nodes, edge_index=edge_index_cpu, edge_type=edge_type_cpu),\n",
    "    num_neighbors=[-1, -1],\n",
    "    batch_size=INFER_BATCH,\n",
    "    input_nodes=torch.arange(num_nodes),\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "with torch.no_grad():\n",
    "    for batch in infer_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        n_id = batch.n_id\n",
    "        z_batch = model(n_id, batch.edge_index, batch.edge_type)\n",
    "        z_all[n_id] = z_batch\n",
    "        del batch, z_batch, n_id\n",
    "        if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Evaluation (per relation + overall) — DistMult + zero_division=0\n",
    "# ----------------------------\n",
    "print(\"\\n📊 Final evaluation per edge type (filtered negatives, DistMult):\")\n",
    "\n",
    "def eval_relation(z_all, etype, splits, k_neg=1):\n",
    "    rel_id = rel2id[etype]\n",
    "    pos_e = splits[\"test\"]\n",
    "    if pos_e.numel() == 0: return None\n",
    "    N = pos_e.size(1)\n",
    "\n",
    "    # Build typed negatives globally (tails only for simplicity)\n",
    "    src_g = pos_e[0].to(device); dst_g = pos_e[1].to(device)\n",
    "    rel_ids = torch.full((N,), rel_id, dtype=torch.long, device=device)\n",
    "    s_type, _, d_type = etype\n",
    "    d_pool_global = torch.arange(type_offsets[d_type],\n",
    "                                 type_offsets[d_type] + len(node_type_map[d_type]),\n",
    "                                 device=device)\n",
    "    neg_src = src_g.repeat_interleave(k_neg)\n",
    "    neg_dst = d_pool_global[torch.randint(0, d_pool_global.numel(), (N * k_neg,), device=device)]\n",
    "\n",
    "    # Scores\n",
    "    pos_scores = distmult_sigmoid_scores(z_all, model.rel_emb, pos_e.to(device), rel_ids)\n",
    "    neg_scores = distmult_sigmoid_scores(z_all, model.rel_emb,\n",
    "                                         torch.stack([neg_src, neg_dst]),\n",
    "                                         rel_ids.repeat_interleave(k_neg))\n",
    "\n",
    "    y_true = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
    "    y_scores = torch.cat([pos_scores, neg_scores]).detach().cpu().numpy()\n",
    "    y = y_true.detach().cpu().numpy()\n",
    "    auc = roc_auc_score(y, y_scores)\n",
    "\n",
    "    preds = (y_scores > 0.5).astype(int)\n",
    "    p = precision_score(y, preds, zero_division=0)\n",
    "    r = recall_score(y, preds, zero_division=0)\n",
    "    f1 = f1_score(y, preds, zero_division=0)\n",
    "    return auc, p, r, f1\n",
    "\n",
    "results = []\n",
    "all_pos_eidx_list, all_rel_ids_list = [], []\n",
    "\n",
    "for etype, splits in edge_type_splits.items():\n",
    "    out = eval_relation(z_all, etype, splits, k_neg=1)\n",
    "    if out is None: continue\n",
    "    auc, p, r, f1 = out\n",
    "    results.append((etype, auc, p, r, f1))\n",
    "    rel_id = rel2id[etype]\n",
    "    te = splits[\"test\"]\n",
    "    all_pos_eidx_list.append(te)\n",
    "    all_rel_ids_list.append(torch.full((te.size(1),), rel_id, dtype=torch.long))\n",
    "    print(f\"{etype}: AUC={auc:.4f}, P={p:.3f}, R={r:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# Overall\n",
    "if all_pos_eidx_list:\n",
    "    all_pos_eidx = torch.cat(all_pos_eidx_list, dim=1).to(device)\n",
    "    all_rel_ids = torch.cat(all_rel_ids_list, dim=0).to(device)\n",
    "    # Simple overall negatives: corrupt tails with matching type (1 per pos)\n",
    "    etypes_for_all = [relid_to_etype(int(r)) for r in all_rel_ids.tolist()]\n",
    "    neg_src = all_pos_eidx[0]\n",
    "    neg_dst_list = []\n",
    "    for r_id, et in zip(all_rel_ids.tolist(), etypes_for_all):\n",
    "        d_type = et[2]\n",
    "        pool = torch.arange(type_offsets[d_type],\n",
    "                            type_offsets[d_type] + len(node_type_map[d_type]),\n",
    "                            device=device)\n",
    "        neg_dst_list.append(pool[torch.randint(0, pool.numel(), (1,), device=device)])\n",
    "    neg_dst = torch.stack(neg_dst_list).view(-1)\n",
    "    neg_e = torch.stack([neg_src, neg_dst], dim=0)\n",
    "\n",
    "    pos_scores = distmult_sigmoid_scores(z_all, model.rel_emb, all_pos_eidx, all_rel_ids)\n",
    "    neg_scores = distmult_sigmoid_scores(z_all, model.rel_emb, neg_e, all_rel_ids)\n",
    "\n",
    "    y_scores = torch.cat([pos_scores, neg_scores]).detach().cpu().numpy()\n",
    "    y = np.concatenate([np.ones(pos_scores.numel()), np.zeros(neg_scores.numel())])\n",
    "\n",
    "    overall_auc = roc_auc_score(y, y_scores)\n",
    "    preds = (y_scores > 0.5).astype(int)\n",
    "    overall_p = precision_score(y, preds, zero_division=0)\n",
    "    overall_r = recall_score(y, preds, zero_division=0)\n",
    "    overall_f1 = f1_score(y, preds, zero_division=0)\n",
    "\n",
    "    print(\"\\n🌍 Overall Performance (filtered-ish, typed tails, DistMult):\")\n",
    "    print(f\"Overall AUC = {overall_auc:.4f}\")\n",
    "    print(f\"Overall Precision = {overall_p:.3f}\")\n",
    "    print(f\"Overall Recall = {overall_r:.3f}\")\n",
    "    print(f\"Overall F1 Score = {overall_f1:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo test edges available for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d95c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "📥 Loading NetworkX graph from pickle: /home/ubuntu/myproject/venv/merged_ofda_hetionet/combined_openfda_hetionet.pkl\n",
      "✅ Loaded graph with 146759 nodes and 1401041 edges.\n",
      "✅ Node types: ['anatomy', 'biological_process', 'cellular_component', 'drug', 'disease', 'gene/protein', 'molecular_function', 'pathway', 'effect/phenotype', 'unknown', 'exposure']\n",
      "✅ Total edge types: 22\n",
      "Homogeneous nodes: 146759, edges used for training: 1120824\n",
      "\n",
      "🔁 Training with NeighborLoader mini-batches + AMP ...\n",
      "Epoch 01 | Loss: 18.0152 | Seen pos edges: 106496\n",
      "Epoch 02 | Loss: 17.8158 | Seen pos edges: 106496\n",
      "Epoch 03 | Loss: 17.2473 | Seen pos edges: 106496\n",
      "Epoch 04 | Loss: 16.5903 | Seen pos edges: 106496\n",
      "Epoch 05 | Loss: 16.0013 | Seen pos edges: 106496\n",
      "Epoch 06 | Loss: 15.4070 | Seen pos edges: 106496\n",
      "Epoch 07 | Loss: 15.0670 | Seen pos edges: 106496\n",
      "Epoch 08 | Loss: 14.7634 | Seen pos edges: 106496\n",
      "Epoch 09 | Loss: 14.4800 | Seen pos edges: 106496\n",
      "Epoch 10 | Loss: 14.3489 | Seen pos edges: 106496\n",
      "Epoch 11 | Loss: 14.2180 | Seen pos edges: 106496\n",
      "Epoch 12 | Loss: 14.0697 | Seen pos edges: 106496\n",
      "Epoch 13 | Loss: 14.0020 | Seen pos edges: 106496\n",
      "Epoch 14 | Loss: 13.9267 | Seen pos edges: 106496\n",
      "Epoch 15 | Loss: 13.8979 | Seen pos edges: 106496\n",
      "Epoch 16 | Loss: 13.8416 | Seen pos edges: 106496\n",
      "Epoch 17 | Loss: 13.7850 | Seen pos edges: 106496\n",
      "Epoch 18 | Loss: 13.7529 | Seen pos edges: 106496\n",
      "Epoch 19 | Loss: 13.6732 | Seen pos edges: 106496\n",
      "Epoch 20 | Loss: 13.6622 | Seen pos edges: 106496\n",
      "Epoch 21 | Loss: 13.6267 | Seen pos edges: 106496\n",
      "Epoch 22 | Loss: 13.6074 | Seen pos edges: 106496\n",
      "Epoch 23 | Loss: 13.5317 | Seen pos edges: 106496\n",
      "Epoch 24 | Loss: 13.4946 | Seen pos edges: 106496\n",
      "Epoch 25 | Loss: 13.5406 | Seen pos edges: 106496\n",
      "Epoch 26 | Loss: 13.4615 | Seen pos edges: 106496\n",
      "Epoch 27 | Loss: 13.4469 | Seen pos edges: 106496\n",
      "Epoch 28 | Loss: 13.4128 | Seen pos edges: 106496\n",
      "Epoch 29 | Loss: 13.4051 | Seen pos edges: 106496\n",
      "Epoch 30 | Loss: 13.3755 | Seen pos edges: 106496\n",
      "Epoch 31 | Loss: 13.3283 | Seen pos edges: 106496\n",
      "Epoch 32 | Loss: 13.3243 | Seen pos edges: 106496\n",
      "Epoch 33 | Loss: 13.2649 | Seen pos edges: 106496\n",
      "Epoch 34 | Loss: 13.2561 | Seen pos edges: 106496\n",
      "Epoch 35 | Loss: 13.1696 | Seen pos edges: 106496\n",
      "Epoch 36 | Loss: 13.2092 | Seen pos edges: 106496\n",
      "Epoch 37 | Loss: 13.1526 | Seen pos edges: 106496\n",
      "Epoch 38 | Loss: 13.1791 | Seen pos edges: 106496\n",
      "Epoch 39 | Loss: 13.1002 | Seen pos edges: 106496\n",
      "Epoch 40 | Loss: 13.1059 | Seen pos edges: 106496\n",
      "\n",
      "🔎 Computing full-node embeddings (batched inference) ...\n",
      "\n",
      "📊 Final evaluation per edge type (filtered negatives, DistMult):\n",
      "('anatomy', 'anatomy_gene', 'gene/protein'): AUC=0.6516, P=0.561, R=0.864, F1=0.680\n",
      "('drug', 'drug_effect', 'effect/phenotype'): AUC=0.5298, P=0.522, R=0.268, F1=0.354\n",
      "('drug', 'drug_exposure', 'exposure'): AUC=0.4965, P=0.495, R=0.191, F1=0.276\n",
      "('drug', 'drug_drug_similarity', 'drug'): AUC=0.9828, P=0.982, R=0.902, F1=0.940\n",
      "('drug', 'drug_disease', 'disease'): AUC=0.7953, P=0.870, R=0.674, F1=0.760\n",
      "('drug', 'drug_gene', 'gene/protein'): AUC=0.9147, P=0.854, R=0.848, F1=0.851\n",
      "('drug', 'drug_has_component', 'drug'): AUC=0.6515, P=0.666, R=0.399, F1=0.499\n",
      "('drug', 'contraindication', 'disease'): AUC=0.4818, P=0.500, R=1.000, F1=0.667\n",
      "('disease', 'disease_gene', 'gene/protein'): AUC=0.6605, P=0.569, R=0.778, F1=0.657\n",
      "('disease', 'disease_phenotype', 'effect/phenotype'): AUC=0.4937, P=0.500, R=1.000, F1=0.667\n",
      "('disease', 'disease_anatomy', 'anatomy'): AUC=0.5361, P=0.516, R=0.551, F1=0.533\n",
      "('disease', 'disease_disease', 'disease'): AUC=0.6327, P=0.549, R=0.709, F1=0.619\n",
      "('gene/protein', 'protein_protein', 'gene/protein'): AUC=0.8184, P=0.755, R=0.719, F1=0.737\n",
      "('gene/protein', 'gene_gene', 'gene/protein'): AUC=0.8091, P=0.852, R=0.544, F1=0.664\n",
      "('gene/protein', 'gene_pathway', 'pathway'): AUC=0.5001, P=0.500, R=0.623, F1=0.555\n",
      "('gene/protein', 'gene_regulates_gene', 'gene/protein'): AUC=0.9206, P=0.892, R=0.744, F1=0.811\n",
      "('effect/phenotype', 'indication', 'effect/phenotype'): AUC=0.5078, P=0.500, R=1.000, F1=0.667\n",
      "('effect/phenotype', 'drug_effect', 'effect/phenotype'): AUC=0.3878, P=0.500, R=1.000, F1=0.667\n",
      "('effect/phenotype', 'contraindication', 'effect/phenotype'): AUC=0.5000, P=0.000, R=0.000, F1=0.000\n",
      "('drug', 'indication', 'disease'): AUC=1.0000, P=0.000, R=0.000, F1=0.000\n",
      "\n",
      "🌍 Overall Performance (filtered-ish, typed tails, DistMult):\n",
      "Overall AUC = 0.7401\n",
      "Overall Precision = 0.638\n",
      "Overall Recall = 0.732\n",
      "Overall F1 Score = 0.682\n"
     ]
    }
   ],
   "source": [
    "# === Pickle (NetworkX) → R-GCN (NeighborLoader + AMP + batched inference) ===\n",
    "# Fixes:\n",
    "#  - DistMult scoring (relation-aware) in train + eval\n",
    "#  - Typed negatives sampled *within the batch* (local), avoiding drop\n",
    "#  - zero_division=0 to silence undefined metric warnings\n",
    "\n",
    "import os, gc, random, pickle, gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "import networkx as nx\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Repro + Device\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gc.collect()\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load PICKLE (NetworkX) + normalize attrs\n",
    "# ----------------------------\n",
    "# Accepts a pickle produced via: pickle.dump(G, open(path,'wb')) where G is a NetworkX graph\n",
    "pickle_path = \"/home/ubuntu/myproject/venv/merged_ofda_hetionet/combined_openfda_hetionet.pkl\"  # <- change if needed\n",
    "assert os.path.exists(pickle_path), f\"Path does not exist: {pickle_path}\"\n",
    "print(f\"📥 Loading NetworkX graph from pickle: {pickle_path}\")\n",
    "\n",
    "open_fn = gzip.open if pickle_path.endswith(\".gz\") else open\n",
    "with open_fn(pickle_path, \"rb\") as f:\n",
    "    G0 = pickle.load(f)\n",
    "\n",
    "# Ensure MultiDiGraph container\n",
    "if isinstance(G0, (nx.MultiDiGraph, nx.MultiGraph, nx.Graph, nx.DiGraph)):\n",
    "    G = nx.MultiDiGraph()\n",
    "    G.add_nodes_from(G0.nodes(data=True))\n",
    "    if isinstance(G0, (nx.MultiDiGraph, nx.MultiGraph)):\n",
    "        for u, v, k, d in G0.edges(keys=True, data=True):\n",
    "            G.add_edge(u, v, key=k, **(d or {}))\n",
    "    else:\n",
    "        for u, v, d in G0.edges(data=True):\n",
    "            G.add_edge(u, v, **(d or {}))\n",
    "else:\n",
    "    raise TypeError(f\"Pickle did not contain a NetworkX graph, got {type(G0)}\")\n",
    "\n",
    "# Normalize expected attributes\n",
    "for n, data in G.nodes(data=True):\n",
    "    if \"node_type\" not in data:\n",
    "        data[\"node_type\"] = data.get(\"type\", data.get(\"category\", data.get(\"kind\", \"unknown\")))\n",
    "for _, _, attr in G.edges(data=True):\n",
    "    if \"relation\" not in attr:\n",
    "        attr[\"relation\"] = attr.get(\"display_relation\", attr.get(\"relationship\", attr.get(\"type\", attr.get(\"label\", \"unknown\"))))\n",
    "\n",
    "print(f\"✅ Loaded graph with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Typed remap + splits\n",
    "# ----------------------------\n",
    "node_type_map = defaultdict(list)\n",
    "node_id_map = {}\n",
    "for nid, data in G.nodes(data=True):\n",
    "    ntype = data.get(\"node_type\", \"unknown\")\n",
    "    idx = len(node_type_map[ntype])\n",
    "    node_type_map[ntype].append(nid)\n",
    "    node_id_map[nid] = (ntype, idx)\n",
    "\n",
    "edge_type_map = defaultdict(list)  # (src_type, rel, dst_type) -> [(src_local, dst_local)]\n",
    "for s, t, attr in G.edges(data=True):\n",
    "    if s not in node_id_map or t not in node_id_map:\n",
    "        continue\n",
    "    rel = attr.get(\"relation\", \"unknown\")\n",
    "    s_t, s_i = node_id_map[s]\n",
    "    t_t, t_i = node_id_map[t]\n",
    "    edge_type_map[(s_t, rel, t_t)].append((s_i, t_i))\n",
    "\n",
    "print(f\"✅ Node types: {list(node_type_map.keys())}\")\n",
    "print(f\"✅ Total edge types: {len(edge_type_map)}\")\n",
    "\n",
    "# Global ids per (type, local)\n",
    "type_offsets, global_id_map = {}, {}\n",
    "offset = 0\n",
    "for ntype, nodes in node_type_map.items():\n",
    "    type_offsets[ntype] = offset\n",
    "    for i in range(len(nodes)):\n",
    "        global_id_map[(ntype, i)] = offset + i\n",
    "    offset += len(nodes)\n",
    "num_nodes = offset\n",
    "\n",
    "edge_type_keys = list(edge_type_map.keys())  # rel_id -> (src_type, rel, dst_type)\n",
    "rel2id = {k: i for i, k in enumerate(edge_type_keys)}\n",
    "num_relations = len(edge_type_keys)\n",
    "\n",
    "all_src, all_dst, all_rel = [], [], []\n",
    "edge_type_splits = {}\n",
    "\n",
    "for etype, pairs in edge_type_map.items():\n",
    "    s_type, _, t_type = etype\n",
    "    if len(pairs) < 5:  # keep only relations with minimum support\n",
    "        continue\n",
    "    perm = torch.randperm(len(pairs))\n",
    "    n_tr = int(0.8 * len(pairs))\n",
    "    n_va = int(0.1 * len(pairs))\n",
    "    tr = [pairs[i] for i in perm[:n_tr]]\n",
    "    va = [pairs[i] for i in perm[n_tr:n_tr + n_va]]\n",
    "    te = [pairs[i] for i in perm[n_tr + n_va:]]\n",
    "\n",
    "    r = rel2id[etype]\n",
    "    for s_l, t_l in tr:\n",
    "        all_src.append(type_offsets[s_type] + s_l)\n",
    "        all_dst.append(type_offsets[t_type] + t_l)\n",
    "        all_rel.append(r)\n",
    "\n",
    "    edge_type_splits[etype] = {\n",
    "        \"val\": torch.tensor(\n",
    "            [[type_offsets[s_type] + s for s, _ in va],\n",
    "             [type_offsets[t_type] + t for _, t in va]], dtype=torch.long),\n",
    "        \"test\": torch.tensor(\n",
    "            [[type_offsets[s_type] + s for s, _ in te],\n",
    "             [type_offsets[t_type] + t for _, t in te]], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "edge_index_cpu = torch.tensor([all_src, all_dst], dtype=torch.long)\n",
    "edge_type_cpu = torch.tensor(all_rel, dtype=torch.long)\n",
    "print(f\"Homogeneous nodes: {num_nodes}, edges used for training: {edge_index_cpu.size(1)}\")\n",
    "\n",
    "# For quick type lookup by GLOBAL id (used for local typed negatives)\n",
    "type_names = sorted(node_type_map.keys())\n",
    "type2id = {t: i for i, t in enumerate(type_names)}\n",
    "global_node_type_id = torch.empty(num_nodes, dtype=torch.long)\n",
    "for tname, nodes in node_type_map.items():\n",
    "    t_id = type2id[tname]\n",
    "    base = type_offsets[tname]\n",
    "    n = len(nodes)\n",
    "    global_node_type_id[base:base + n] = t_id\n",
    "\n",
    "# Existing edges per relation (LOCAL to each (src_type,rel,dst_type))\n",
    "existing_edges_per_rel = {etype: set(pairs) for etype, pairs in edge_type_map.items()}\n",
    "def relid_to_etype(rel_id: int):\n",
    "    return edge_type_keys[rel_id]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) PyG Data + NeighborLoader\n",
    "# ----------------------------\n",
    "data_pyg = Data(num_nodes=num_nodes, edge_index=edge_index_cpu, edge_type=edge_type_cpu)\n",
    "\n",
    "NUM_NEIGHBORS = [15, 10]\n",
    "NBATCH_SIZE   = 4096\n",
    "NUM_WORKERS   = 2  # quiets the OS warning\n",
    "\n",
    "train_src_unique = torch.unique(edge_index_cpu[0])\n",
    "train_loader = NeighborLoader(\n",
    "    data_pyg,\n",
    "    num_neighbors=NUM_NEIGHBORS,\n",
    "    batch_size=NBATCH_SIZE,\n",
    "    input_nodes=train_src_unique,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Model (DistMult) + helpers\n",
    "# ----------------------------\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, num_relations, emb_dim=64, hidden_dim=128, out_dim=32, num_bases=30):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_nodes, emb_dim)\n",
    "        self.conv1 = RGCNConv(emb_dim, hidden_dim, num_relations=num_relations, num_bases=num_bases)\n",
    "        self.conv2 = RGCNConv(hidden_dim, out_dim, num_relations=num_relations, num_bases=num_bases)\n",
    "        self.rel_emb = nn.Embedding(num_relations, out_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
    "\n",
    "    def forward(self, n_id, edge_index, edge_type):\n",
    "        # n_id: global ids ordered as local nodes in the subgraph\n",
    "        x = self.emb(n_id)\n",
    "        x = self.conv1(x, edge_index, edge_type).relu_()\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x  # local node embeddings\n",
    "\n",
    "    def score(self, h, r, t):\n",
    "        # DistMult (logits)\n",
    "        return (h * r * t).sum(dim=-1)\n",
    "\n",
    "def distmult_sigmoid_scores(z, rel_emb, eidx, rel_ids):\n",
    "    h = z[eidx[0]]; t = z[eidx[1]]\n",
    "    r = rel_emb(rel_ids)\n",
    "    return (h * r * t).sum(dim=-1).sigmoid()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Local (in-batch) typed negatives\n",
    "# ----------------------------\n",
    "def build_local_type_pools(n_id_device):\n",
    "    \"\"\"Map node_type_id -> list of LOCAL indices present in this batch.\"\"\"\n",
    "    g_types = global_node_type_id.to(n_id_device.device, non_blocking=True)[n_id_device]\n",
    "    pools = defaultdict(list)\n",
    "    for local_idx, t in enumerate(g_types.tolist()):\n",
    "        pools[t].append(local_idx)\n",
    "    for k in list(pools.keys()):\n",
    "        pools[k] = torch.tensor(pools[k], dtype=torch.long, device=n_id_device.device)\n",
    "    return pools\n",
    "\n",
    "def sample_neg_local(pos_local_eidx, rel_ids_local, n_id, pools, k_neg=1):\n",
    "    \"\"\"\n",
    "    Typed negatives *within* the sampled subgraph.\n",
    "    pos_local_eidx: (2,P) local indices\n",
    "    rel_ids_local:  (P,)   relation ids (global rel ids)\n",
    "    pools: dict[type_id] -> local node index tensor present in batch\n",
    "    \"\"\"\n",
    "    if pos_local_eidx.numel() == 0:  # empty\n",
    "        return torch.empty((2,0), dtype=torch.long, device=pos_local_eidx.device)\n",
    "\n",
    "    P = pos_local_eidx.size(1)\n",
    "    neg_src, neg_dst = [], []\n",
    "    for i in range(P):\n",
    "        s_l = int(pos_local_eidx[0, i])\n",
    "        d_l = int(pos_local_eidx[1, i])\n",
    "        r_id = int(rel_ids_local[i])\n",
    "        s_type, _, d_type = relid_to_etype(r_id)\n",
    "        s_tid = type2id[s_type]; d_tid = type2id[d_type]\n",
    "\n",
    "        # pick from local pools — if missing (rare), fall back to uniform local sampling\n",
    "        s_pool = pools.get(s_tid, None)\n",
    "        d_pool = pools.get(d_tid, None)\n",
    "        if s_pool is None or s_pool.numel() == 0:\n",
    "            s_pool = torch.arange(n_id.size(0), device=n_id.device)\n",
    "        if d_pool is None or d_pool.numel() == 0:\n",
    "            d_pool = torch.arange(n_id.size(0), device=n_id.device)\n",
    "\n",
    "        for _ in range(max(1, k_neg)):\n",
    "            if random.random() < 0.5:\n",
    "                # corrupt head\n",
    "                new_s_l = s_pool[torch.randint(0, s_pool.numel(), (1,))].item()\n",
    "                neg_src.append(new_s_l); neg_dst.append(d_l)\n",
    "            else:\n",
    "                # corrupt tail\n",
    "                new_d_l = d_pool[torch.randint(0, d_pool.numel(), (1,))].item()\n",
    "                neg_src.append(s_l); neg_dst.append(new_d_l)\n",
    "\n",
    "    if len(neg_src) == 0:\n",
    "        return torch.empty((2,0), dtype=torch.long, device=n_id.device)\n",
    "    return torch.stack([torch.tensor(neg_src, device=n_id.device),\n",
    "                        torch.tensor(neg_dst, device=n_id.device)], dim=0)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Train with NeighborLoader + AMP\n",
    "# ----------------------------\n",
    "EPOCHS = 40\n",
    "LR     = 1e-3\n",
    "K_NEG  = 1\n",
    "\n",
    "model = RGCN(num_nodes, num_relations, emb_dim=64, hidden_dim=128, out_dim=32, num_bases=30).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "# Global training edges (for picking positives per batch)\n",
    "src_glob_all = edge_index_cpu[0].to(device)\n",
    "dst_glob_all = edge_index_cpu[1].to(device)\n",
    "rels_all     = edge_type_cpu.to(device)\n",
    "\n",
    "print(\"\\n🔁 Training with NeighborLoader mini-batches + AMP ...\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    seen_pos = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        n_id = batch.n_id  # global ids in this subgraph (device tensor)\n",
    "\n",
    "        # global->local mapping\n",
    "        global2local = -torch.ones(num_nodes, dtype=torch.long, device=device)\n",
    "        global2local[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # positives inside batch\n",
    "        in_src = global2local[src_glob_all]\n",
    "        in_dst = global2local[dst_glob_all]\n",
    "        mask = (in_src >= 0) & (in_dst >= 0)\n",
    "        pos_idx = torch.nonzero(mask, as_tuple=False).view(-1)\n",
    "        if pos_idx.numel() == 0:\n",
    "            del batch, n_id, global2local, in_src, in_dst, mask, pos_idx\n",
    "            if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        MAX_POS_PER_BATCH = 8192\n",
    "        if pos_idx.numel() > MAX_POS_PER_BATCH:\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device)[:MAX_POS_PER_BATCH]]\n",
    "\n",
    "        pos_local = torch.stack([in_src[pos_idx], in_dst[pos_idx]], dim=0)\n",
    "        rel_ids_local = rels_all[pos_idx]\n",
    "\n",
    "        # local typed pools for negatives\n",
    "        pools = build_local_type_pools(n_id)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=(device.type == \"cuda\")):\n",
    "            z = model(n_id, batch.edge_index, batch.edge_type)\n",
    "\n",
    "            # DistMult positives\n",
    "            pos_h = z[pos_local[0]]; pos_t = z[pos_local[1]]\n",
    "            pos_r = model.rel_emb(rel_ids_local)\n",
    "            pos_logits = model.score(pos_h, pos_r, pos_t)\n",
    "\n",
    "            # local typed negatives\n",
    "            neg_local = sample_neg_local(pos_local, rel_ids_local, n_id, pools, k_neg=K_NEG)\n",
    "            if neg_local.size(1) > 0:\n",
    "                neg_h = z[neg_local[0]]; neg_t = z[neg_local[1]]\n",
    "                # repeat relation ids for negs\n",
    "                neg_r = model.rel_emb(rel_ids_local.repeat_interleave(K_NEG))\n",
    "                neg_logits = model.score(neg_h, neg_r, neg_t)\n",
    "                loss = (F.binary_cross_entropy_with_logits(pos_logits, torch.ones_like(pos_logits)) +\n",
    "                        F.binary_cross_entropy_with_logits(neg_logits, torch.zeros_like(neg_logits)))\n",
    "            else:\n",
    "                loss = F.binary_cross_entropy_with_logits(pos_logits, torch.ones_like(pos_logits))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += float(loss.detach().cpu())\n",
    "        seen_pos += pos_local.size(1)\n",
    "\n",
    "        # cleanup\n",
    "        del batch, n_id, global2local, in_src, in_dst, mask, pos_idx\n",
    "        del pos_local, rel_ids_local, pools, z, pos_h, pos_t, pos_r, pos_logits\n",
    "        if 'neg_local' in locals(): del neg_local, neg_h, neg_t, neg_r, neg_logits\n",
    "        if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {epoch_loss:.4f} | Seen pos edges: {seen_pos}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Batched inference (DistMult-ready embeddings)\n",
    "# ----------------------------\n",
    "print(\"\\n🔎 Computing full-node embeddings (batched inference) ...\")\n",
    "model.eval()\n",
    "OUT_DIM = 32\n",
    "z_all = torch.empty((num_nodes, OUT_DIM), dtype=torch.float32, device=device)\n",
    "\n",
    "INFER_BATCH = 16384\n",
    "infer_loader = NeighborLoader(\n",
    "    Data(num_nodes=num_nodes, edge_index=edge_index_cpu, edge_type=edge_type_cpu),\n",
    "    num_neighbors=[-1, -1],\n",
    "    batch_size=INFER_BATCH,\n",
    "    input_nodes=torch.arange(num_nodes),\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "with torch.no_grad():\n",
    "    for batch in infer_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        n_id = batch.n_id\n",
    "        z_batch = model(n_id, batch.edge_index, batch.edge_type)\n",
    "        z_all[n_id] = z_batch\n",
    "        del batch, z_batch, n_id\n",
    "        if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Evaluation (per relation + overall) — DistMult + zero_division=0\n",
    "# ----------------------------\n",
    "print(\"\\n📊 Final evaluation per edge type (filtered negatives, DistMult):\")\n",
    "\n",
    "def eval_relation(z_all, etype, splits, k_neg=1):\n",
    "    rel_id = rel2id[etype]\n",
    "    pos_e = splits[\"test\"]\n",
    "    if pos_e.numel() == 0: return None\n",
    "    N = pos_e.size(1)\n",
    "\n",
    "    # Build typed negatives globally (tails only for simplicity)\n",
    "    src_g = pos_e[0].to(device); dst_g = pos_e[1].to(device)\n",
    "    rel_ids = torch.full((N,), rel_id, dtype=torch.long, device=device)\n",
    "    s_type, _, d_type = etype\n",
    "    d_pool_global = torch.arange(type_offsets[d_type],\n",
    "                                 type_offsets[d_type] + len(node_type_map[d_type]),\n",
    "                                 device=device)\n",
    "    neg_src = src_g.repeat_interleave(k_neg)\n",
    "    neg_dst = d_pool_global[torch.randint(0, d_pool_global.numel(), (N * k_neg,), device=device)]\n",
    "\n",
    "    # Scores\n",
    "    pos_scores = distmult_sigmoid_scores(z_all, model.rel_emb, pos_e.to(device), rel_ids)\n",
    "    neg_scores = distmult_sigmoid_scores(z_all, model.rel_emb,\n",
    "                                         torch.stack([neg_src, neg_dst]),\n",
    "                                         rel_ids.repeat_interleave(k_neg))\n",
    "\n",
    "    y_true = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
    "    y_scores = torch.cat([pos_scores, neg_scores]).detach().cpu().numpy()\n",
    "    y = y_true.detach().cpu().numpy()\n",
    "    auc = roc_auc_score(y, y_scores)\n",
    "\n",
    "    preds = (y_scores > 0.5).astype(int)\n",
    "    p = precision_score(y, preds, zero_division=0)\n",
    "    r = recall_score(y, preds, zero_division=0)\n",
    "    f1 = f1_score(y, preds, zero_division=0)\n",
    "    return auc, p, r, f1\n",
    "\n",
    "results = []\n",
    "all_pos_eidx_list, all_rel_ids_list = [], []\n",
    "\n",
    "for etype, splits in edge_type_splits.items():\n",
    "    out = eval_relation(z_all, etype, splits, k_neg=1)\n",
    "    if out is None: continue\n",
    "    auc, p, r, f1 = out\n",
    "    results.append((etype, auc, p, r, f1))\n",
    "    rel_id = rel2id[etype]\n",
    "    te = splits[\"test\"]\n",
    "    all_pos_eidx_list.append(te)\n",
    "    all_rel_ids_list.append(torch.full((te.size(1),), rel_id, dtype=torch.long))\n",
    "    print(f\"{etype}: AUC={auc:.4f}, P={p:.3f}, R={r:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# Overall\n",
    "if all_pos_eidx_list:\n",
    "    all_pos_eidx = torch.cat(all_pos_eidx_list, dim=1).to(device)\n",
    "    all_rel_ids = torch.cat(all_rel_ids_list, dim=0).to(device)\n",
    "    # Simple overall negatives: corrupt tails with matching type (1 per pos)\n",
    "    etypes_for_all = [relid_to_etype(int(r)) for r in all_rel_ids.tolist()]\n",
    "    neg_src = all_pos_eidx[0]\n",
    "    neg_dst_list = []\n",
    "    for r_id, et in zip(all_rel_ids.tolist(), etypes_for_all):\n",
    "        d_type = et[2]\n",
    "        pool = torch.arange(type_offsets[d_type],\n",
    "                            type_offsets[d_type] + len(node_type_map[d_type]),\n",
    "                            device=device)\n",
    "        neg_dst_list.append(pool[torch.randint(0, pool.numel(), (1,), device=device)])\n",
    "    neg_dst = torch.stack(neg_dst_list).view(-1)\n",
    "    neg_e = torch.stack([neg_src, neg_dst], dim=0)\n",
    "\n",
    "    pos_scores = distmult_sigmoid_scores(z_all, model.rel_emb, all_pos_eidx, all_rel_ids)\n",
    "    neg_scores = distmult_sigmoid_scores(z_all, model.rel_emb, neg_e, all_rel_ids)\n",
    "\n",
    "    y_scores = torch.cat([pos_scores, neg_scores]).detach().cpu().numpy()\n",
    "    y = np.concatenate([np.ones(pos_scores.numel()), np.zeros(neg_scores.numel())])\n",
    "\n",
    "    overall_auc = roc_auc_score(y, y_scores)\n",
    "    preds = (y_scores > 0.5).astype(int)\n",
    "    overall_p = precision_score(y, preds, zero_division=0)\n",
    "    overall_r = recall_score(y, preds, zero_division=0)\n",
    "    overall_f1 = f1_score(y, preds, zero_division=0)\n",
    "\n",
    "    print(\"\\n🌍 Overall Performance (filtered-ish, typed tails, DistMult):\")\n",
    "    print(f\"Overall AUC = {overall_auc:.4f}\")\n",
    "    print(f\"Overall Precision = {overall_p:.3f}\")\n",
    "    print(f\"Overall Recall = {overall_r:.3f}\")\n",
    "    print(f\"Overall F1 Score = {overall_f1:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo test edges available for evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

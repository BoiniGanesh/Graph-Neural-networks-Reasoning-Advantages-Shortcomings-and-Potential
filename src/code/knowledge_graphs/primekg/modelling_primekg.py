# -*- coding: utf-8 -*-
"""Modelling_primekg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Buj-7NfHhmApDpERicOzoCecIJ_-1WxO
"""

!pip install torch==2.3.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

!pip install torch-scatter torch-sparse pyg-lib torch-geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html

from google.colab import drive
drive.mount('/content/drive')

# ‚úÖ PrimeKG R-GCN Modeling Pipeline

import pickle
import torch
import torch.nn.functional as F
from torch import nn
from torch_geometric.nn import RGCNConv
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
from collections import defaultdict
import numpy as np
import random

# --- Load Pickled PrimeKG Graph ---
pkl_path = "/content/drive/MyDrive/primekg_graph.pkl"
with open(pkl_path, "rb") as f:
    G = pickle.load(f)
print(f"‚úÖ Loaded graph with {len(G.nodes)} nodes and {len(G.edges)} edges.")

# --- Map Nodes and Edges Using PrimeKG Naming Conventions ---
node_type_map = defaultdict(list)
node_id_map = {}
node_idx_by_type = defaultdict(dict)

for nid, data in G.nodes(data=True):
    ntype = data.get("node_type", "unknown")
    idx = len(node_type_map[ntype])
    node_type_map[ntype].append(nid)
    node_id_map[nid] = (ntype, idx)
    node_idx_by_type[ntype][nid] = idx

edge_type_map = defaultdict(list)
for src, dst, attr in G.edges(data=True):
    rel = attr.get("relation", "unknown")
    if src not in node_id_map or dst not in node_id_map:
        continue
    src_t, src_i = node_id_map[src]
    dst_t, dst_i = node_id_map[dst]
    edge_type_map[(src_t, rel, dst_t)].append((src_i, dst_i))

print(f"‚úÖ Node types: {list(node_type_map.keys())}")
print(f"‚úÖ Total edge types: {len(edge_type_map)}")

# --- Build Homogeneous Graph ---
type_offsets = {}
global_id_map = {}
offset = 0
for ntype, nodes in node_type_map.items():
    type_offsets[ntype] = offset
    for local_idx in range(len(nodes)):
        global_id_map[(ntype, local_idx)] = offset + local_idx
    offset += len(nodes)

num_nodes = offset
edge_type_keys = list(edge_type_map.keys())
rel2id = {etype: i for i, etype in enumerate(edge_type_keys)}
num_relations = len(edge_type_keys)

all_src, all_dst, all_rel = [], [], []
edge_type_splits = {}

for etype, edges in edge_type_map.items():
    rel_id = rel2id[etype]
    src_type, _, dst_type = etype
    if len(edges) < 5:
        continue
    perm = torch.randperm(len(edges))
    num_train = int(0.8 * len(edges))
    num_val = int(0.1 * len(edges))
    train_edges = [edges[i] for i in perm[:num_train]]
    val_edges = [edges[i] for i in perm[num_train:num_train+num_val]]
    test_edges = [edges[i] for i in perm[num_train+num_val:]]

    for (s_local, d_local) in train_edges:
        all_src.append(type_offsets[src_type] + s_local)
        all_dst.append(type_offsets[dst_type] + d_local)
        all_rel.append(rel_id)

    edge_type_splits[etype] = {
        "val": torch.tensor([
            [type_offsets[src_type] + s for s, d in val_edges],
            [type_offsets[dst_type] + d for s, d in val_edges]
        ], dtype=torch.long),
        "test": torch.tensor([
            [type_offsets[src_type] + s for s, d in test_edges],
            [type_offsets[dst_type] + d for s, d in test_edges]
        ], dtype=torch.long)
    }

edge_index = torch.tensor([all_src, all_dst], dtype=torch.long)
edge_type = torch.tensor(all_rel, dtype=torch.long)

print(f"Homogeneous nodes: {num_nodes}, edges used for training: {edge_index.size(1)}")

# --- R-GCN Model Definition ---
class RGCN(torch.nn.Module):
    def __init__(self, num_nodes, num_relations, emb_dim=64, hidden_dim=64, out_dim=32):
        super().__init__()
        self.emb = torch.nn.Embedding(num_nodes, emb_dim)
        self.conv1 = RGCNConv(emb_dim, hidden_dim, num_relations=num_relations)
        self.conv2 = RGCNConv(hidden_dim, out_dim, num_relations=num_relations)

    def forward(self, node_ids, edge_index, edge_type):
        x = self.emb(node_ids)
        x = self.conv1(x, edge_index, edge_type)
        x = F.relu(x)
        x = self.conv2(x, edge_index, edge_type)
        return x

# --- Helper Functions ---
def edge_scores(z, eidx):
    return (z[eidx[0]] * z[eidx[1]]).sum(dim=-1).sigmoid()

def sample_neg_edges(num_samples, num_nodes, device):
    return torch.randint(0, num_nodes, (2, num_samples), device=device)

@torch.no_grad()
def evaluate(z, pos_eidx, num_nodes, k_neg=1):
    num_pos = pos_eidx.size(1)
    neg_eidx = sample_neg_edges(num_pos * k_neg, num_nodes, z.device)

    y_true = torch.cat([
        torch.ones(num_pos, device=z.device),
        torch.zeros(num_pos * k_neg, device=z.device)
    ])
    scores = torch.cat([edge_scores(z, pos_eidx),
                        edge_scores(z, neg_eidx)], dim=0)

    y_true_cpu = y_true.detach().cpu().numpy()
    scores_cpu = scores.detach().cpu().numpy()
    auc = roc_auc_score(y_true_cpu, scores_cpu)
    preds = (scores_cpu > 0.5).astype(int)
    precision = precision_score(y_true_cpu, preds)
    recall = recall_score(y_true_cpu, preds)
    f1 = f1_score(y_true_cpu, preds)
    return auc, precision, recall, f1

# --- Unified R-GCN Training ---
print("\nüîÅ Training unified R-GCN model on all edge types together...")
EPOCHS = 30
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
node_ids = torch.arange(num_nodes, device=device)
model = RGCN(num_nodes, num_relations).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# --- Convert train edges to TensorDataset for batching ---
import torch.utils.data as tud

BATCH_SIZE = 100_000
train_edges = torch.tensor([all_src, all_dst], dtype=torch.long)
train_rels = torch.tensor(all_rel, dtype=torch.long)

train_dataset = tud.TensorDataset(train_edges[0], train_edges[1], train_rels)
train_loader = tud.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

# --- Mini-batched Training Loop ---
for epoch in range(1, EPOCHS + 1):
    model.train()
    epoch_loss = 0

    for src, dst, rel in train_loader:
        optimizer.zero_grad()

        z = model(node_ids, edge_index.to(device), edge_type.to(device))  # unified graph
        pos_eidx = torch.stack([src, dst]).to(device)

        pos_scores = edge_scores(z, pos_eidx)
        pos_loss = -torch.log(pos_scores + 1e-15).mean()

        neg_eidx = sample_neg_edges(pos_eidx.size(1), num_nodes, device)
        neg_scores = edge_scores(z, neg_eidx)
        neg_loss = -torch.log(1.0 - neg_scores + 1e-15).mean()

        loss = pos_loss + neg_loss
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    print(f"Epoch {epoch:02d} | Total Loss: {epoch_loss:.4f}")

# --- Final Evaluation Per Edge Type ---
print("\nüìä Final evaluation per edge type:")
model.eval()
results = []
with torch.no_grad():
    z = model(node_ids, edge_index.to(device), edge_type.to(device))
    for edge_type_key, splits in edge_type_splits.items():
        test_edges = splits["test"].to(device)
        auc, p, r, f1 = evaluate(z, test_edges, num_nodes)
        results.append({
            "edge_type": edge_type_key,
            "auc": auc,
            "precision": p,
            "recall": r,
            "f1": f1
        })
        print(f"{edge_type_key}: AUC={auc:.4f}, P={p:.3f}, R={r:.3f}, F1={f1:.3f}")

# --- Overall Metrics ---
print("\nüåç Overall Performance:")
all_test_edges = torch.cat([splits["test"] for splits in edge_type_splits.values()], dim=1)
auc, p, r, f1 = evaluate(z, all_test_edges, num_nodes)
print(f"Overall AUC = {auc:.4f}")
print(f"Overall Precision = {p:.3f}")
print(f"Overall Recall = {r:.3f}")
print(f"Overall F1 Score = {f1:.3f}")

